{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3e09a2-f821-4fcd-9a20-9816c5062a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\olanr\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olanr\\anaconda3\\envs\\telegram_bot_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from test_sematic_chunking import pdf_path, extract_book_text, chunk_pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2869bf-9325-46c2-87d5-1977dd8d47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_content = extract_book_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "045e3d16-30e6-4aff-9225-5b24631e864c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-02 18:01:39 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|██████████| 337/337 [03:51<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_pdf_content(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c3ad0b-0351-49f4-9bb6-5eba51c952e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If you have',\n",
       " 'purchased a license to use this document from IIBA',\n",
       " '®, you may transfer ownership to a third party.',\n",
       " 'IIBA® members may not',\n",
       " 'transfer ownership of their complimentary copy.',\n",
       " 'This document is provided to the busine ss analysis community for educational purpos es.',\n",
       " 'IIBA® does not warrant that it is',\n",
       " 'suitable for any other purpose and makes no expressed or impl ied warranty of any kind and assumes no responsibility for',\n",
       " 'errors or omissions.',\n",
       " 'No liability is assumed for incidental or consequential damages in connection with or arising out of the',\n",
       " 'use of the information contained herein.IIBA',\n",
       " '®, the IIBA® logo, BABOK® and Business Analysis Body of Knowledge® are registered trademarks owned by',\n",
       " 'International Institute of Business Analysis.',\n",
       " 'CBAP® is a registered certification mark owned by International Institute of',\n",
       " 'Business Analysis.',\n",
       " 'Certified Business Analysis Professional,  EEP and the EEP logo are trademarks owned by International',\n",
       " 'Institute of Business Analysis.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][1].splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9961eecd-183a-4835-bec2-c997b021bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e836ee6-1c41-41bb-9bc7-15dd907fc103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1042829"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbb8ecdd-1b88-483f-90f8-e00b613a7da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038945"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "309a29ba-82ef-4984-91fb-e034e5ee6ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd5c3681-2374-43eb-acec-771638ecf6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-02 10:18:09 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|██████████| 432/432 [06:34<00:00,  1.10it/s]  \n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_pdf_content(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a25a5d66-24b3-4bff-81cf-836c391ab22b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks[0][1].splits\n",
    "from typing import List, Dict\n",
    "def create_chunk_mapping(chunks: List)-> Dict[str, List]:\n",
    "    if not chunks[0] or not chunks:\n",
    "        print(\"Chunks could not be created\")\n",
    "        return {\"chunk\": \"No chunks created\"}\n",
    "\n",
    "    chunk_map = {}\n",
    "    for idx, chunk in enumerate(chunks[0]):\n",
    "        chunk_map[f\"chunk_{idx}\"] = \" \".join(chunk.splits)\n",
    "\n",
    "    return chunk_map\n",
    "\n",
    "\n",
    "chunk_map = create_chunk_mapping(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "674ee883-fb4c-44d3-ad7e-eb40def992b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A  G U I D E  T O  T H E  B U S I N E S S  A N A LY S I S B O DY  O F  K N O W L E D G E ® v3 BABOK ® v3 A GUIDE TO THE BUSINESS ANALYSIS BODY OF KNOWLEDGE® Complimentary IIBA® Member Copy. Not for Distribution or Resale. International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis. All rights reserved. Version 1.0 and 1.4 published 2005. Version 1.6 Draft published 2006. Version 1.6 Final published 2008. Version 2.0 published 2009. Version 3.0 published 2015. ISBN-13: 978-1-927584-03-3 Permission is granted to reproduce this document for your own personal, professional, or educational use.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_map['chunk_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee2281b1-2435-48b4-a8e5-ebc23744fa31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4632\\118013625.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0msearch_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;31m#\"International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis\"#\"A  G U I D E  T O  T H E  B U S I N E S S  A N A LY S I S B O DY  O F  K N O W L E D G E ® v3 BABOK ® v3 A GUIDE TO THE BUSINESS ANALYSIS BODY OF KNOWLEDGE® Complimentary IIBA® Member Copy. Not for Distribution or Resale. International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis. All rights reserved. Version 1.0 and 1.4 published 2005. Version 1.6 Draft published 2006. Version 1.6 Final published 2008. Version 2.0 published 2009. Version 3.0 published 2015. ISBN-13: 978-1-927584-03-3 Permission is granted to reproduce this document for your own personal, professional, or educational use.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#pages = get_text_to_page_map(pdf_path, search_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts_to_page_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mText found on pages: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4632\\118013625.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path, chunks)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mchunk_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_texts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0msearch_texts_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_to_page_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mchunk_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_texts_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunk_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4632\\118013625.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(pdf_path, search_texts)\u001b[0m\n\u001b[0;32m     36\u001b[0m                         \u001b[0mpage_numbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# if search_text.lower() in page_text.lower():  # Case-insensitive search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m#     page_numbers.append(page_num + 1)  # PDF page numbers are 1-based\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mPDF Filepath: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m does not exist\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\site-packages\\pymupdf\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(page, option, clip, flags, textpage, sort, delimiters)\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractXML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0moption\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"xhtml\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractXHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtextpage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sort)\u001b[0m\n\u001b[0;32m  12609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12610\u001b[0m         \u001b[1;34m\"\"\"Return simple, bare text on the page.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12612\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  12613\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractBLOCKS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12614\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, format_)\u001b[0m\n\u001b[0;32m  12414\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mformat_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12415\u001b[0m             \u001b[0mmupdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfz_print_stext_page_as_xhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_tpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12417\u001b[0m             \u001b[0mJM_print_stext_page_as_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_tpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12418\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfz_close_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  12419\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJM_EscapeStrFromBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12420\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\site-packages\\pymupdf\\mupdf.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[1;32m> 26629\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mfz_close_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  26630\u001b[0m         r\"\"\"\n\u001b[0;32m  26631\u001b[0m         \u001b[0mClass\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0maware\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m`\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfz_close_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  26632\u001b[0m                 \u001b[0mFlush\u001b[0m \u001b[0mpending\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mclose\u001b[0m \u001b[0man\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_combos(subtexts: list)-> list:\n",
    "    all_combos = []\n",
    "    \n",
    "    for offset in range(1, len(subtexts) + 1):\n",
    "        for idx in range(len(subtexts)):\n",
    "            possible_combo = []\n",
    "            if idx + offset == len(subtexts):\n",
    "                break\n",
    "            \n",
    "            current_sentence = \" \".join(subtexts[idx])\n",
    "            next_sentence = \" \".join(subtexts[idx:idx + offset + 1])\n",
    "            possible_combo.append(current_sentence + \" \" + next_sentence)\n",
    "            all_combos.extend(possible_combo)\n",
    "        \n",
    "    return all_combos\n",
    "\n",
    "\n",
    "def get_text_to_page_map(pdf_path: str, search_texts: list) -> tuple:\n",
    "    page_numbers = []\n",
    "\n",
    "    #search_texts = get_combos(search_texts)\n",
    "    search_texts_map = {}\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            # Loop through all pages and search for the text\n",
    "            for page_num in range(len(pdf)):\n",
    "                page = pdf.load_page(page_num)\n",
    "                page_text = page.get_text()\n",
    "            \n",
    "                for sentence in search_texts:\n",
    "                    if sentence.lower() in page_text.lower():\n",
    "                        page_numbers.append(page_num + 1)\n",
    "                        \n",
    "                # if search_text.lower() in page_text.lower():  # Case-insensitive search\n",
    "                #     page_numbers.append(page_num + 1)  # PDF page numbers are 1-based\n",
    "    except FileNotFoundError:\n",
    "        print(f\"PDF Filepath: {pdf_path} does not exist\")\n",
    "        return []\n",
    "\n",
    "    if Counter(page_numbers).most_common(1):\n",
    "        search_texts_map[\" \".join(search_texts)] = Counter(page_numbers).most_common(1)[0][0]\n",
    "        return search_texts_map\n",
    "    else:\n",
    "        search_texts_map[\" \".join(search_texts)] = -1\n",
    "        return search_texts_map\n",
    "\n",
    "\n",
    "def texts_to_page_mapping(pdf_path: str, chunks:list)-> dict:\n",
    "    if not chunks or not chunks[0]:\n",
    "        return {}\n",
    "\n",
    "    chunk_mapping = {}\n",
    "    for idx, search_texts in enumerate(chunks[0]):\n",
    "        search_texts_map = get_text_to_page_map(pdf_path, search_texts.splits)\n",
    "        chunk_mapping.update(search_texts_map)\n",
    "\n",
    "    return chunk_mapping\n",
    "\n",
    "\n",
    "# Example usage\n",
    "search_text = chunks[0][100].splits#\"International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis\"#\"A  G U I D E  T O  T H E  B U S I N E S S  A N A LY S I S B O DY  O F  K N O W L E D G E ® v3 BABOK ® v3 A GUIDE TO THE BUSINESS ANALYSIS BODY OF KNOWLEDGE® Complimentary IIBA® Member Copy. Not for Distribution or Resale. International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis. All rights reserved. Version 1.0 and 1.4 published 2005. Version 1.6 Draft published 2006. Version 1.6 Final published 2008. Version 2.0 published 2009. Version 3.0 published 2015. ISBN-13: 978-1-927584-03-3 Permission is granted to reproduce this document for your own personal, professional, or educational use.\"\n",
    "#pages = get_text_to_page_map(pdf_path, search_text)\n",
    "pages = texts_to_page_mapping(pdf_path, chunks)\n",
    "print(f\"Text found on pages: {pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f455ede-ab80-4912-a938-870c8c503895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors. Other stakeholders ma y not see value in the work, may misunderstand the value being provided, or may be concerned about the effect the change will have on them. Stakeholders who are expected to serve in key roles and participate heavily in business an alysis activities, but who view a change negatively, may require collaboration appr oaches that increas e their cooperation. Decision Making AuthorityBusiness analysts identify the authority level a stakeholder possesses over business analysis activities, deli verables, and changes to business analysis work. Plan Stakeholder Engagement Business Analysis Planning and Monitoring': 43}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56e69917-4ab3-404c-844f-d5e9c47c09cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 45\u001b[0m, in \u001b[0;36msearch_texts_on_all_pages\u001b[1;34m(pdf_path, search_texts)\u001b[0m\n\u001b[0;32m     43\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(get_text_to_page_map, pdf_path, search_texts, page_num) \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_pages)]\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[0;32m     46\u001b[0m     page_num, page_numbers \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\concurrent\\futures\\_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[1;34m(fs, timeout)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[1;32m--> 243\u001b[0m waiter\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mwait(wait_timeout)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    356\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 71\u001b[0m\n\u001b[0;32m     66\u001b[0m         chunk_mapping\u001b[38;5;241m.\u001b[39mupdate(search_texts_map)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk_mapping\n\u001b[1;32m---> 71\u001b[0m pages \u001b[38;5;241m=\u001b[39m texts_to_page_mapping(pdf_path, chunks)\n",
      "Cell \u001b[1;32mIn[94], line 65\u001b[0m, in \u001b[0;36mtexts_to_page_mapping\u001b[1;34m(pdf_path, chunks)\u001b[0m\n\u001b[0;32m     63\u001b[0m chunk_mapping \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, search_texts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 65\u001b[0m     search_texts_map \u001b[38;5;241m=\u001b[39m search_texts_on_all_pages(pdf_path, search_texts\u001b[38;5;241m.\u001b[39msplits)\n\u001b[0;32m     66\u001b[0m     chunk_mapping\u001b[38;5;241m.\u001b[39mupdate(search_texts_map)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunk_mapping\n",
      "Cell \u001b[1;32mIn[94], line 41\u001b[0m, in \u001b[0;36msearch_texts_on_all_pages\u001b[1;34m(pdf_path, search_texts)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Use ThreadPoolExecutor to search on multiple pages concurrently\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Create a future for each page, passing the page number and search_texts\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(get_text_to_page_map, pdf_path, search_texts, page_num) \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_pages)]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\concurrent\\futures\\_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\concurrent\\futures\\thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[1;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[1;32m--> 238\u001b[0m         t\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[0;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "\n",
    "def get_text_to_page_map(pdf_path: str, search_texts: list, page_num: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Searches for a list of search_texts on a given page number.\n",
    "    Returns a tuple with page number and any matches found on that page.\n",
    "    \"\"\"\n",
    "    page_numbers = []\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            page = pdf.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            for sentence in search_texts:\n",
    "                if sentence.lower() in page_text.lower():\n",
    "                    page_numbers.append(page_num + 1)  # PDF page numbers are 1-based\n",
    "    except FileNotFoundError:\n",
    "        print(f\"PDF Filepath: {pdf_path} does not exist\")\n",
    "        return page_num, []\n",
    "\n",
    "    return page_num, page_numbers\n",
    "\n",
    "def search_texts_on_all_pages(pdf_path: str, search_texts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Concurrently searches for search_texts on all pages of the PDF.\n",
    "    Returns a mapping of the text to the page numbers where they are found.\n",
    "    \"\"\"\n",
    "    search_texts_map = {}\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            total_pages = len(pdf)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"PDF Filepath: {pdf_path} does not exist\")\n",
    "        return {}\n",
    "\n",
    "    # Use ThreadPoolExecutor to search on multiple pages concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Create a future for each page, passing the page number and search_texts\n",
    "        futures = [executor.submit(get_text_to_page_map, pdf_path, search_texts, page_num) for page_num in range(total_pages)]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            page_num, page_numbers = future.result()\n",
    "            \n",
    "            # Track which page had matches\n",
    "            if page_numbers:\n",
    "                for sentence in search_texts:\n",
    "                    if sentence.lower() in page_numbers:\n",
    "                        search_texts_map[sentence] = page_num\n",
    "\n",
    "    return search_texts_map\n",
    "\n",
    "def texts_to_page_mapping(pdf_path: str, chunks: list) -> dict:\n",
    "    \"\"\"\n",
    "    Maps the chunks of search text to the corresponding pages in the PDF.\n",
    "    \"\"\"\n",
    "    if not chunks or not chunks[0]:\n",
    "        return {}\n",
    "\n",
    "    chunk_mapping = {}\n",
    "    for idx, search_texts in enumerate(chunks[0]):\n",
    "        search_texts_map = search_texts_on_all_pages(pdf_path, search_texts.splits)\n",
    "        chunk_mapping.update(search_texts_map)\n",
    "\n",
    "    return chunk_mapping\n",
    "\n",
    "\n",
    "pages = texts_to_page_mapping(pdf_path, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1206da31-af30-42ce-92b7-48f601c7bf45",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m         chunk_mapping\u001b[38;5;241m.\u001b[39mupdate(search_texts_map)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk_mapping\n\u001b[1;32m---> 70\u001b[0m pages \u001b[38;5;241m=\u001b[39m texts_to_page_mapping(pdf_path, chunks)\n",
      "Cell \u001b[1;32mIn[96], line 65\u001b[0m, in \u001b[0;36mtexts_to_page_mapping\u001b[1;34m(pdf_path, chunks)\u001b[0m\n\u001b[0;32m     63\u001b[0m chunk_mapping \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, search_texts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 65\u001b[0m     search_texts_map \u001b[38;5;241m=\u001b[39m search_texts_on_all_pages(pdf_path, search_texts\u001b[38;5;241m.\u001b[39msplits)\n\u001b[0;32m     66\u001b[0m     chunk_mapping\u001b[38;5;241m.\u001b[39mupdate(search_texts_map)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunk_mapping\n",
      "Cell \u001b[1;32mIn[96], line 46\u001b[0m, in \u001b[0;36msearch_texts_on_all_pages\u001b[1;34m(pdf_path, search_texts)\u001b[0m\n\u001b[0;32m     43\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(get_text_to_page_map, pdf_path, search_texts, page_num) \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_pages)]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m---> 46\u001b[0m     page_num, page_numbers \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Track which page had matches\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page_numbers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\telegram_bot_env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "\n",
    "def get_text_to_page_map(pdf_path: str, search_texts: list, page_num: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Searches for a list of search_texts on a given page number.\n",
    "    Returns a tuple with page number and any matches found on that page.\n",
    "    \"\"\"\n",
    "    page_numbers = []\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            page = pdf.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            for sentence in search_texts:\n",
    "                if sentence.lower() in page_text.lower():\n",
    "                    page_numbers.append(page_num + 1)  # PDF page numbers are 1-based\n",
    "    except FileNotFoundError:\n",
    "        print(f\"PDF Filepath: {pdf_path} does not exist\")\n",
    "        return page_num, []\n",
    "\n",
    "    return page_num, page_numbers\n",
    "\n",
    "def search_texts_on_all_pages(pdf_path: str, search_texts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Concurrently searches for search_texts on all pages of the PDF using multiprocessing.\n",
    "    Returns a mapping of the text to the page numbers where they are found.\n",
    "    \"\"\"\n",
    "    search_texts_map = {}\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            total_pages = len(pdf)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"PDF Filepath: {pdf_path} does not exist\")\n",
    "        return {}\n",
    "\n",
    "    # Use ProcessPoolExecutor for multiprocessing\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Create a future for each page, passing the page number and search_texts\n",
    "        futures = [executor.submit(get_text_to_page_map, pdf_path, search_texts, page_num) for page_num in range(total_pages)]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            page_num, page_numbers = future.result()\n",
    "            \n",
    "            # Track which page had matches\n",
    "            if page_numbers:\n",
    "                for sentence in search_texts:\n",
    "                    if sentence.lower() in page_numbers:\n",
    "                        search_texts_map[sentence] = page_num\n",
    "\n",
    "    return search_texts_map\n",
    "\n",
    "def texts_to_page_mapping(pdf_path: str, chunks: list) -> dict:\n",
    "    \"\"\"\n",
    "    Maps the chunks of search text to the corresponding pages in the PDF using multiprocessing.\n",
    "    \"\"\"\n",
    "    if not chunks or not chunks[0]:\n",
    "        return {}\n",
    "\n",
    "    chunk_mapping = {}\n",
    "    for idx, search_texts in enumerate(chunks[0]):\n",
    "        search_texts_map = search_texts_on_all_pages(pdf_path, search_texts.splits)\n",
    "        chunk_mapping.update(search_texts_map)\n",
    "\n",
    "    return chunk_mapping\n",
    "\n",
    "pages = texts_to_page_mapping(pdf_path, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe5709b7-01fa-4f33-899c-996e935c255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chunk(splits=['A GUIDE TO THE BUSINESS ANALYSIS', 'BODY OF KNOWLEDGE®v3BABOK®', 'v3', 'A GUIDE TO THE BUSINESS ANALYSIS', 'BODY OF KNOWLEDGE®Complimentary IIBA® Member Copy.', 'Not for Distribution or Resale.International Institute of Business Analysis, Toronto, Ontario, Canada.', '©2005, 2006, 2008, 2009, 2015 International Institut e of Business Analysis.', 'All rights reserved.', 'Version 1.0 and 1.4 published 2005.', 'Version 1.6 Draft publ ished 2006.', 'Version 1.6 Final published 2008.', 'Version 2.0', 'published 2009.', 'Version 3.0 published 2015.ISBN-13: 978-1-927584-03-3', 'Permission is granted to reproduce this document for your own personal, professi onal, or educational use.'], is_triggered=True, triggered_score=0.1981200617928198, token_count=188, metadata=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25177d21-e0e8-4bbe-b668-f66707f6ee63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['® Guide .',\n",
       " 'For',\n",
       " \"example, when performing 'P lan Business Analysis Information Management' it\",\n",
       " 'includes all the examples  listed above.',\n",
       " \"If the BABOK® Guide  described 'Plan\",\n",
       " \"Requirements Management', it would excl ude important outputs like elicitation\",\n",
       " 'results, solution options, and change strategy.',\n",
       " 'DesignA design is a usable representation of a solution.',\n",
       " 'Design focuses on',\n",
       " 'understanding how value might be realized by  a solution if it is built.',\n",
       " 'The nature',\n",
       " 'of the representation may be a document (or set of documents) and can vary widely depending on the circumstances.',\n",
       " 'Enterprise',\n",
       " 'An enterprise is a system of one or more  organizations and the solutions they use',\n",
       " 'to pursue a shared set of common goals.',\n",
       " 'These solutions (also referred to as',\n",
       " 'organizational capabilities) can be proc esses, tools or information.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20b7c8f3-a21b-4ac8-a847-ba142e5c990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combos(subtexts: list)-> list:\n",
    "    all_combos = []\n",
    "    \n",
    "    for offset in range(1, len(subtexts) + 1):\n",
    "        for idx in range(len(subtexts)):\n",
    "            possible_combo = []\n",
    "            if idx + offset == len(subtexts):\n",
    "                break\n",
    "            \n",
    "            current_sentence = \" \".join(subtexts[idx])\n",
    "            next_sentence = \" \".join(subtexts[idx:idx + offset + 1])\n",
    "            possible_combo.append(current_sentence + \" \" + next_sentence)\n",
    "            all_combos.extend(possible_combo)\n",
    "        \n",
    "    return all_combos\n",
    "\n",
    "x = get_combos(search_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f6602-8925-4768-930b-0ed416a09e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e15960be-4e4f-450f-9d5a-a4367cef1c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I f   t h e   B A B O K ®   G u i d e     d e s c r i b e d   ' P l a n If the BABOK® Guide  described 'Plan Requirements Management', it would excl ude important outputs like elicitation results, solution options, and change strategy.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf3cbd-adc0-42ec-91d3-6404ca5ae693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "\n",
    "def split_search_text(search_text: str, delimiter: str = r\"\\.\") -> list:\n",
    "    \"\"\"\n",
    "    Split the search text into smaller sentences using a given delimiter (default is period).\n",
    "    \"\"\"\n",
    "    # Split by period and remove empty entries\n",
    "    sentences = re.split(delimiter, search_text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def search_sentence_in_page(args):\n",
    "    \"\"\"\n",
    "    Search for a sentence in a single page.\n",
    "    Args is a tuple (pdf_path, page_num, sentence).\n",
    "    \"\"\"\n",
    "    pdf_path, page_num, sentence = args\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        page = pdf.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        if sentence.lower() in page_text.lower():\n",
    "            return page_num + 1  # Return 1-based page number if sentence is found\n",
    "    return None\n",
    "\n",
    "def search_text_across_pages(pdf_path: str, search_text: str, num_processes: int = 4) -> list:\n",
    "    \"\"\"\n",
    "    Search for sentences in a PDF using parallel processing, allowing for sentences to spill across pages.\n",
    "    \"\"\"\n",
    "    # Split search text by sentences (you can change the delimiter to suit your needs)\n",
    "    sentences = split_search_text(search_text, delimiter=r\"\\.|,|\\?|!\")  # Split by periods, commas, question marks, etc.\n",
    "    \n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf)\n",
    "    \n",
    "    # Prepare arguments for each page and sentence\n",
    "    search_results = []\n",
    "    for sentence in sentences:\n",
    "        page_args = [(pdf_path, page_num, sentence) for page_num in range(total_pages)]\n",
    "        \n",
    "        # Use multiprocessing Pool to parallelize the text search for each sentence\n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            results = pool.map(search_sentence_in_page, page_args)\n",
    "        \n",
    "        # Filter out None values and store the results for each sentence\n",
    "        found_pages = [page_num for page_num in results if page_num is not None]\n",
    "        search_results.append((sentence, found_pages))\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "def print_search_results(search_results):\n",
    "    \"\"\"\n",
    "    Print the search results for each sentence.\n",
    "    \"\"\"\n",
    "    for sentence, pages in search_results:\n",
    "        if pages:\n",
    "            print(f\"Sentence: \\\"{sentence}\\\" found on pages: {pages}\")\n",
    "        else:\n",
    "            print(f\"Sentence: \\\"{sentence}\\\" not found in the document.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "search_text = chunk_map['chunk_0']#\"Some long text that might spill over. Another sentence?\"\n",
    "search_results = search_text_across_pages(pdf_path, search_text, num_processes=4)\n",
    "print_search_results(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc2947-1a18-49ba-9c17-83ad2e6376e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f831588e-eef7-47d1-9e9f-57619c48df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def split_search_text(search_text: str, delimiter: str = r\"\\.\") -> list:\n",
    "    \"\"\"\n",
    "    Split the search text into smaller sentences using a given delimiter (default is period).\n",
    "    \"\"\"\n",
    "    # Split by period and remove empty entries\n",
    "    sentences = re.split(delimiter, search_text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def search_sentence_in_page(pdf_path: str, page_num: int, sentence: str) -> bool:\n",
    "    \"\"\"\n",
    "    Search for a sentence in a single page.\n",
    "    Returns True if the sentence is found, False otherwise.\n",
    "    \"\"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        page = pdf.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        # Check if the sentence exists in the page's text\n",
    "        return sentence.lower() in page_text.lower()\n",
    "\n",
    "def search_text_across_pages(pdf_path: str, search_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Search for sentences in a PDF without using parallel processing.\n",
    "    \"\"\"\n",
    "    # Split search text by sentences (you can change the delimiter to suit your needs)\n",
    "    sentences = split_search_text(search_text, delimiter=r\".\")  # Split by periods, commas, question marks, etc.\n",
    "    \n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf)\n",
    "    \n",
    "    # Iterate over each sentence and each page to search\n",
    "    search_results = []\n",
    "    for sentence in sentences:\n",
    "        found_pages = []\n",
    "        for page_num in range(total_pages):\n",
    "            if search_sentence_in_page(pdf_path, page_num, sentence):\n",
    "                found_pages.append(page_num + 1)  # Store 1-based page number\n",
    "        search_results.append((sentence, found_pages))\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "def print_search_results(search_results):\n",
    "    \"\"\"\n",
    "    Print the search results for each sentence.\n",
    "    \"\"\"\n",
    "    for sentence, pages in search_results:\n",
    "        if pages:\n",
    "            print(f\"Sentence: \\\"{sentence}\\\" found on pages: {pages}\")\n",
    "        else:\n",
    "            print(f\"Sentence: \\\"{sentence}\\\" not found in the document.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "t = \"\"\"A  G U I D E  T O  T H E  B U S I N E S S  A N A LY S I S B O DY  O F  K N O W L E D G E ® v3 BABOK ® v3 A GUIDE TO THE BUSINESS ANALYSIS BODY OF KNOWLEDGE® Complimentary IIBA® Member Copy. Not for Distribution or Resale. International Institute of Business Analysis, Toronto, Ontario, Canada. ©2005, 2006, 2008, 2009, 2015 International Institute of Business Analysis. All rights reserved. Version 1.0 and 1.4 published 2005. Version 1.6 Draft published 2006. Version 1.6 Final published 2008. Version 2.0 published 2009. Version 3.0 published 2015. ISBN-13: 978-1-927584-03-3 Permission is granted to reproduce this document for your own personal, professional, or educational use.\"\"\"\n",
    "search_text = t#\"Some long text that might spill over. Another sentence?\"\n",
    "search_results = search_text_across_pages(pdf_path, search_text)\n",
    "print_search_results(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7195e308-9bcf-4766-a34e-de89b0a1f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "168027fa-84c5-4957-8863-7c45fb40e9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you have purchased a license to use this document from IIBA®, you may transfer ownership to a third party. IIBA® members may not transfer ownership of their complimentary copy. This document is provided to the business analysis community for educational purposes. IIBA® does not warrant that it is suitable for any other purpose and makes no expressed or implied warranty of any kind and assumes no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information contained herein. IIBA®, the IIBA® logo, BABOK® and Business Analysis Body of Knowledge® are registered trademarks owned by International Institute of Business Analysis. CBAP® is a registered certification mark owned by International Institute of Business Analysis. Certified Business Analysis Professional, EEP and the EEP logo are trademarks owned by International Institute of Business Analysis. Archimate® is a registered trademark of The Open Group in the US and other countries. Business Model Canvas is copyrighted by BusinessModelGeneration.com and released under Creative Commons license. CMMI® is a registered trademark of Carnegie Mellon University. COBIT® is a trademark of the Information Systems Audit and Control Association and the IT Governance Institute. Mind Map® is a registered trademark of the Buzan Organization. Scaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(chunks[0][1].splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
